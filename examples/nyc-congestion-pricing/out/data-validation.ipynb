{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c2c377",
   "metadata": {
    "papermill": {
     "duration": 0.005284,
     "end_time": "2026-01-01T19:18:10.163970",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.158686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Validation Report\n",
    "\n",
    "This notebook validates the NYC Yellow Taxi trip data and provides:\n",
    "- **Completeness Check**: Identifies missing months in the dataset\n",
    "- **Schema Validation**: Ensures consistency across all Parquet files\n",
    "- **Data Quality Metrics**: Row counts, null values, and data ranges\n",
    "- **Schema Documentation**: Detailed field descriptions for use in other notebooks\n",
    "\n",
    "Run this notebook before performing analysis to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de77afdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:10.175283Z",
     "iopub.status.busy": "2026-01-01T19:18:10.174786Z",
     "iopub.status.idle": "2026-01-01T19:18:10.449790Z",
     "shell.execute_reply": "2026-01-01T19:18:10.448211Z"
    },
    "papermill": {
     "duration": 0.28331,
     "end_time": "2026-01-01T19:18:10.451020",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.167710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2d95f6",
   "metadata": {
    "papermill": {
     "duration": 0.00464,
     "end_time": "2026-01-01T19:18:10.459743",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.455103",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Data Inventory\n",
    "\n",
    "Scan the data directory and catalog all available files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba5c7102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:10.470961Z",
     "iopub.status.busy": "2026-01-01T19:18:10.470231Z",
     "iopub.status.idle": "2026-01-01T19:18:10.479139Z",
     "shell.execute_reply": "2026-01-01T19:18:10.477230Z"
    },
    "papermill": {
     "duration": 0.015843,
     "end_time": "2026-01-01T19:18:10.480176",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.464333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Directory: C:\\Users\\Rob\\Desktop\\ai-for-the-rest\\examples\\nyc-congestion-pricing\\data\n",
      "Pattern: yellow_tripdata_*.parquet\n",
      "Files Found: 59\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "data_dir = Path(\"data\")\n",
    "data_type = \"yellow\"\n",
    "pattern = f\"{data_type}_tripdata_*.parquet\"\n",
    "\n",
    "# Find all data files\n",
    "data_files = sorted(data_dir.glob(pattern))\n",
    "\n",
    "print(f\"Data Directory: {data_dir.absolute()}\")\n",
    "print(f\"Pattern: {pattern}\")\n",
    "print(f\"Files Found: {len(data_files)}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55d579a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:10.487995Z",
     "iopub.status.busy": "2026-01-01T19:18:10.487707Z",
     "iopub.status.idle": "2026-01-01T19:18:10.519926Z",
     "shell.execute_reply": "2026-01-01T19:18:10.518700Z"
    },
    "papermill": {
     "duration": 0.037986,
     "end_time": "2026-01-01T19:18:10.521367",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.483381",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File Inventory (59 files):\n",
      "shape: (59, 2)\n",
      "┌────────────┬───────────┐\n",
      "│ year_month ┆ size_mb   │\n",
      "│ ---        ┆ ---       │\n",
      "│ str        ┆ f64       │\n",
      "╞════════════╪═══════════╡\n",
      "│ 2021-01    ┆ 20.681445 │\n",
      "│ 2021-02    ┆ 20.768412 │\n",
      "│ 2021-03    ┆ 28.617718 │\n",
      "│ 2021-04    ┆ 32.442627 │\n",
      "│ 2021-05    ┆ 36.948854 │\n",
      "│ …          ┆ …         │\n",
      "│ 2025-07    ┆ 63.842514 │\n",
      "│ 2025-08    ┆ 59.407943 │\n",
      "│ 2025-09    ┆ 69.077439 │\n",
      "│ 2025-10    ┆ 71.780766 │\n",
      "│ 2025-11    ┆ 67.838912 │\n",
      "└────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Extract file metadata\n",
    "file_info = []\n",
    "for file_path in data_files:\n",
    "    # Parse year-month from filename\n",
    "    # Format: yellow_tripdata_YYYY-MM.parquet\n",
    "    parts = file_path.stem.split(\"_\")\n",
    "    if len(parts) >= 3:\n",
    "        year_month = parts[2]  # e.g., \"2021-01\"\n",
    "        year, month = map(int, year_month.split(\"-\"))\n",
    "        \n",
    "        file_info.append({\n",
    "            \"filename\": file_path.name,\n",
    "            \"year\": year,\n",
    "            \"month\": month,\n",
    "            \"year_month\": year_month,\n",
    "            \"size_mb\": file_path.stat().st_size / (1024 * 1024),\n",
    "            \"path\": str(file_path)\n",
    "        })\n",
    "\n",
    "# Convert to Polars DataFrame for easy analysis\n",
    "if len(file_info) > 0:\n",
    "    files_df = pl.DataFrame(file_info).sort([\"year\", \"month\"])\n",
    "    print(f\"\\nFile Inventory ({len(files_df)} files):\")\n",
    "    print(files_df.select([\"year_month\", \"size_mb\"]))\n",
    "else:\n",
    "    files_df = pl.DataFrame()\n",
    "    print(\"\\nNo valid data files found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6f5555",
   "metadata": {
    "papermill": {
     "duration": 0.00365,
     "end_time": "2026-01-01T19:18:10.528998",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.525348",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2. Completeness Check\n",
    "\n",
    "Identify missing months in the expected date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df946bcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:10.537456Z",
     "iopub.status.busy": "2026-01-01T19:18:10.537160Z",
     "iopub.status.idle": "2026-01-01T19:18:10.563844Z",
     "shell.execute_reply": "2026-01-01T19:18:10.562632Z"
    },
    "papermill": {
     "duration": 0.033374,
     "end_time": "2026-01-01T19:18:10.565454",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.532080",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Range: 2021-01 to 2025-11\n",
      "\n",
      "Expected Months: 59\n",
      "Available Months: 59\n",
      "Missing Months: 0\n",
      "\n",
      "✓ All expected months are present\n"
     ]
    }
   ],
   "source": [
    "if len(files_df) > 0:\n",
    "    # Determine expected date range\n",
    "    min_year = files_df[\"year\"].min()\n",
    "    min_month = files_df.filter(pl.col(\"year\") == min_year)[\"month\"].min()\n",
    "    max_year = files_df[\"year\"].max()\n",
    "    max_month = files_df.filter(pl.col(\"year\") == max_year)[\"month\"].max()\n",
    "    \n",
    "    print(f\"Date Range: {min_year}-{min_month:02d} to {max_year}-{max_month:02d}\")\n",
    "    \n",
    "    # Generate expected months\n",
    "    start_date = datetime(min_year, min_month, 1)\n",
    "    end_date = datetime(max_year, max_month, 1)\n",
    "    \n",
    "    expected_months = []\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        expected_months.append(f\"{current.year}-{current.month:02d}\")\n",
    "        # Move to next month\n",
    "        if current.month == 12:\n",
    "            current = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=current.month + 1)\n",
    "    \n",
    "    # Find missing months\n",
    "    available_months = set(files_df[\"year_month\"].to_list())\n",
    "    missing_months = [m for m in expected_months if m not in available_months]\n",
    "    \n",
    "    print(f\"\\nExpected Months: {len(expected_months)}\")\n",
    "    print(f\"Available Months: {len(available_months)}\")\n",
    "    print(f\"Missing Months: {len(missing_months)}\")\n",
    "    \n",
    "    if missing_months:\n",
    "        print(f\"\\n⚠️  ALERT: Missing data for the following months:\")\n",
    "        for month in missing_months:\n",
    "            print(f\"   - {month}\")\n",
    "    else:\n",
    "        print(\"\\n✓ All expected months are present\")\n",
    "else:\n",
    "    print(\"⚠️  No data files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dce734",
   "metadata": {
    "papermill": {
     "duration": 0.004548,
     "end_time": "2026-01-01T19:18:10.576848",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.572300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Schema Validation\n",
    "\n",
    "Verify that all files have consistent schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7b22418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:10.585569Z",
     "iopub.status.busy": "2026-01-01T19:18:10.585202Z",
     "iopub.status.idle": "2026-01-01T19:18:10.601698Z",
     "shell.execute_reply": "2026-01-01T19:18:10.600372Z"
    },
    "papermill": {
     "duration": 0.022504,
     "end_time": "2026-01-01T19:18:10.602845",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.580341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference Schema (from yellow_tripdata_2021-01.parquet):\n",
      "Total Columns: 19\n",
      "\n",
      "shape: (19, 2)\n",
      "┌───────────────────────┬───────────┐\n",
      "│ column_name           ┆ data_type │\n",
      "│ ---                   ┆ ---       │\n",
      "│ str                   ┆ str       │\n",
      "╞═══════════════════════╪═══════════╡\n",
      "│ VendorID              ┆ BIGINT    │\n",
      "│ tpep_pickup_datetime  ┆ TIMESTAMP │\n",
      "│ tpep_dropoff_datetime ┆ TIMESTAMP │\n",
      "│ passenger_count       ┆ DOUBLE    │\n",
      "│ trip_distance         ┆ DOUBLE    │\n",
      "│ …                     ┆ …         │\n",
      "│ tolls_amount          ┆ DOUBLE    │\n",
      "│ improvement_surcharge ┆ DOUBLE    │\n",
      "│ total_amount          ┆ DOUBLE    │\n",
      "│ congestion_surcharge  ┆ DOUBLE    │\n",
      "│ airport_fee           ┆ DOUBLE    │\n",
      "└───────────────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "# Get schema from the first file as reference\n",
    "if len(data_files) > 0:\n",
    "    reference_file = data_files[0]\n",
    "    \n",
    "    # Use DuckDB to get schema information\n",
    "    reference_schema = duckdb.sql(f\"\"\"\n",
    "        SELECT * FROM '{reference_file}' LIMIT 0\n",
    "    \"\"\").description\n",
    "    \n",
    "    schema_columns = [col[0] for col in reference_schema]\n",
    "    schema_types = [col[1] for col in reference_schema]\n",
    "    \n",
    "    print(f\"Reference Schema (from {reference_file.name}):\")\n",
    "    print(f\"Total Columns: {len(schema_columns)}\\n\")\n",
    "    \n",
    "    # Display schema\n",
    "    schema_df = pl.DataFrame({\n",
    "        \"column_name\": schema_columns,\n",
    "        \"data_type\": [str(t) for t in schema_types]\n",
    "    })\n",
    "    print(schema_df)\n",
    "else:\n",
    "    print(\"No files to analyze\")\n",
    "    schema_columns = []\n",
    "    schema_types = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82e1f975",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:10.614390Z",
     "iopub.status.busy": "2026-01-01T19:18:10.613667Z",
     "iopub.status.idle": "2026-01-01T19:18:10.692246Z",
     "shell.execute_reply": "2026-01-01T19:18:10.690129Z"
    },
    "papermill": {
     "duration": 0.087034,
     "end_time": "2026-01-01T19:18:10.693794",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.606760",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking schema consistency across all files...\n",
      "\n",
      "⚠️  ALERT: Found 34 files with schema inconsistencies:\n",
      "\n",
      "   - yellow_tripdata_2023-02.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-03.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-04.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-05.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-06.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-07.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-08.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-09.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-10.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-11.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2023-12.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-01.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-02.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-03.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-04.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-05.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-06.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-07.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-08.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-09.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-10.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-11.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2024-12.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 19\n",
      "   - yellow_tripdata_2025-01.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-02.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-03.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-04.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-05.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-06.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-07.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-08.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-09.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-10.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n",
      "   - yellow_tripdata_2025-11.parquet\n",
      "     Column names mismatch: Expected 19 columns, got 20\n"
     ]
    }
   ],
   "source": [
    "# Check schema consistency across all files\n",
    "if len(data_files) > 1:\n",
    "    print(\"\\nChecking schema consistency across all files...\\n\")\n",
    "    \n",
    "    inconsistent_files = []\n",
    "    \n",
    "    for file_path in data_files[1:]:\n",
    "        file_schema = duckdb.sql(f\"\"\"\n",
    "            SELECT * FROM '{file_path}' LIMIT 0\n",
    "        \"\"\").description\n",
    "        \n",
    "        file_columns = [col[0] for col in file_schema]\n",
    "        file_types = [str(col[1]) for col in file_schema]\n",
    "        \n",
    "        # Check for differences\n",
    "        if file_columns != schema_columns:\n",
    "            inconsistent_files.append({\n",
    "                \"file\": file_path.name,\n",
    "                \"issue\": \"Column names mismatch\",\n",
    "                \"details\": f\"Expected {len(schema_columns)} columns, got {len(file_columns)}\"\n",
    "            })\n",
    "        elif [str(t) for t in schema_types] != file_types:\n",
    "            inconsistent_files.append({\n",
    "                \"file\": file_path.name,\n",
    "                \"issue\": \"Data types mismatch\",\n",
    "                \"details\": \"Column types differ from reference\"\n",
    "            })\n",
    "    \n",
    "    if inconsistent_files:\n",
    "        print(f\"⚠️  ALERT: Found {len(inconsistent_files)} files with schema inconsistencies:\\n\")\n",
    "        for issue in inconsistent_files:\n",
    "            print(f\"   - {issue['file']}\")\n",
    "            print(f\"     {issue['issue']}: {issue['details']}\")\n",
    "    else:\n",
    "        print(\"✓ All files have consistent schemas\")\n",
    "elif len(data_files) == 1:\n",
    "    print(\"Only one file available - cannot check consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88516e",
   "metadata": {
    "papermill": {
     "duration": 0.00519,
     "end_time": "2026-01-01T19:18:10.705634",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.700444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Data Quality Metrics\n",
    "\n",
    "Analyze row counts, null values, and data ranges for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3c7d48b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:10.717226Z",
     "iopub.status.busy": "2026-01-01T19:18:10.716380Z",
     "iopub.status.idle": "2026-01-01T19:18:10.854762Z",
     "shell.execute_reply": "2026-01-01T19:18:10.850927Z"
    },
    "papermill": {
     "duration": 0.146966,
     "end_time": "2026-01-01T19:18:10.856772",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.709806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data quality metrics...\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Statistics:\n",
      "shape: (59, 3)\n",
      "┌────────────┬───────────┬───────────┐\n",
      "│ year_month ┆ row_count ┆ size_mb   │\n",
      "│ ---        ┆ ---       ┆ ---       │\n",
      "│ str        ┆ i64       ┆ f64       │\n",
      "╞════════════╪═══════════╪═══════════╡\n",
      "│ 2021-01    ┆ 1369769   ┆ 20.681445 │\n",
      "│ 2021-02    ┆ 1371709   ┆ 20.768412 │\n",
      "│ 2021-03    ┆ 1925152   ┆ 28.617718 │\n",
      "│ 2021-04    ┆ 2171187   ┆ 32.442627 │\n",
      "│ 2021-05    ┆ 2507109   ┆ 36.948854 │\n",
      "│ …          ┆ …         ┆ …         │\n",
      "│ 2025-07    ┆ 3898963   ┆ 63.842514 │\n",
      "│ 2025-08    ┆ 3574091   ┆ 59.407943 │\n",
      "│ 2025-09    ┆ 4251015   ┆ 69.077439 │\n",
      "│ 2025-10    ┆ 4428699   ┆ 71.780766 │\n",
      "│ 2025-11    ┆ 4181444   ┆ 67.838912 │\n",
      "└────────────┴───────────┴───────────┘\n",
      "\n",
      "================================================================================\n",
      "Summary:\n",
      "  Total Files: 59\n",
      "  Total Rows: 194,457,948\n",
      "  Total Size: 3032.40 MB\n",
      "  Avg Rows/File: 3,295,897\n",
      "  Min Rows: 1,369,769 (2021-01)\n",
      "  Max Rows: 4,591,845 (2025-05)\n"
     ]
    }
   ],
   "source": [
    "# Get row counts and basic statistics for each file\n",
    "if len(data_files) > 0:\n",
    "    print(\"Analyzing data quality metrics...\\n\")\n",
    "    \n",
    "    file_stats = []\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        # Get row count\n",
    "        row_count = duckdb.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as cnt FROM '{file_path}'\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        # Extract year-month\n",
    "        year_month = file_path.stem.split(\"_\")[2]\n",
    "        \n",
    "        file_stats.append({\n",
    "            \"year_month\": year_month,\n",
    "            \"filename\": file_path.name,\n",
    "            \"row_count\": row_count,\n",
    "            \"size_mb\": file_path.stat().st_size / (1024 * 1024)\n",
    "        })\n",
    "    \n",
    "    stats_df = pl.DataFrame(file_stats).sort(\"year_month\")\n",
    "    \n",
    "    print(\"File Statistics:\")\n",
    "    print(stats_df.select([\"year_month\", \"row_count\", \"size_mb\"]))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Summary:\")\n",
    "    print(f\"  Total Files: {len(stats_df)}\")\n",
    "    print(f\"  Total Rows: {stats_df['row_count'].sum():,}\")\n",
    "    print(f\"  Total Size: {stats_df['size_mb'].sum():.2f} MB\")\n",
    "    print(f\"  Avg Rows/File: {stats_df['row_count'].mean():,.0f}\")\n",
    "    print(f\"  Min Rows: {stats_df['row_count'].min():,} ({stats_df.filter(pl.col('row_count') == pl.col('row_count').min())['year_month'][0]})\")\n",
    "    print(f\"  Max Rows: {stats_df['row_count'].max():,} ({stats_df.filter(pl.col('row_count') == pl.col('row_count').max())['year_month'][0]})\")\n",
    "else:\n",
    "    print(\"No files to analyze\")\n",
    "    stats_df = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ac70c9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:10.867210Z",
     "iopub.status.busy": "2026-01-01T19:18:10.866917Z",
     "iopub.status.idle": "2026-01-01T19:18:25.574238Z",
     "shell.execute_reply": "2026-01-01T19:18:25.572373Z"
    },
    "papermill": {
     "duration": 14.714232,
     "end_time": "2026-01-01T19:18:25.576032",
     "exception": false,
     "start_time": "2026-01-01T19:18:10.861800",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking for null values in key columns...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca82e8b479a4b6c8a9868c62caead61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows across all files: 194,457,948\n",
      "\n",
      "Null Value Analysis:\n",
      "shape: (10, 3)\n",
      "┌───────────────────────┬───────────────┬───────────────────────────────┐\n",
      "│ column                ┆ null_count    ┆ null_percentage               │\n",
      "│ ---                   ┆ ---           ┆ ---                           │\n",
      "│ str                   ┆ decimal[38,0] ┆ decimal[38,27]                │\n",
      "╞═══════════════════════╪═══════════════╪═══════════════════════════════╡\n",
      "│ passenger_count       ┆ 18663998      ┆ 9.597960994631085997060917253 │\n",
      "│ VendorID              ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "│ tpep_pickup_datetime  ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "│ tpep_dropoff_datetime ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "│ trip_distance         ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "│ PULocationID          ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "│ DOLocationID          ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "│ payment_type          ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "│ fare_amount           ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "│ total_amount          ┆ 0             ┆ 0.000000000000000000000000000 │\n",
      "└───────────────────────┴───────────────┴───────────────────────────────┘\n",
      "\n",
      "✓ No columns have excessive null values (>10%)\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in key columns\n",
    "if len(data_files) > 0:\n",
    "    print(\"\\nChecking for null values in key columns...\\n\")\n",
    "    \n",
    "    # Query all files at once using glob pattern\n",
    "    pattern_path = str(data_dir / pattern)\n",
    "    \n",
    "    # Key columns to check\n",
    "    key_columns = [\n",
    "        \"VendorID\",\n",
    "        \"tpep_pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\",\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"PULocationID\",\n",
    "        \"DOLocationID\",\n",
    "        \"payment_type\",\n",
    "        \"fare_amount\",\n",
    "        \"total_amount\"\n",
    "    ]\n",
    "    \n",
    "    # Build dynamic query to count nulls\n",
    "    null_checks = []\n",
    "    for col in key_columns:\n",
    "        null_checks.append(f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) as {col}_nulls\")\n",
    "    \n",
    "    null_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        {', '.join(null_checks)}\n",
    "    FROM '{pattern_path}'\n",
    "    \"\"\"\n",
    "    \n",
    "    null_results = duckdb.sql(null_query).pl()\n",
    "    \n",
    "    # Calculate null percentages\n",
    "    total_rows = null_results[\"total_rows\"][0]\n",
    "    print(f\"Total rows across all files: {total_rows:,}\\n\")\n",
    "    \n",
    "    null_summary = []\n",
    "    for col in key_columns:\n",
    "        null_count = null_results[f\"{col}_nulls\"][0]\n",
    "        null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "        null_summary.append({\n",
    "            \"column\": col,\n",
    "            \"null_count\": null_count,\n",
    "            \"null_percentage\": null_pct\n",
    "        })\n",
    "    \n",
    "    null_df = pl.DataFrame(null_summary).sort(\"null_percentage\", descending=True)\n",
    "    print(\"Null Value Analysis:\")\n",
    "    print(null_df)\n",
    "    \n",
    "    # Alert on high null percentages\n",
    "    high_nulls = null_df.filter(pl.col(\"null_percentage\") > 10)\n",
    "    if len(high_nulls) > 0:\n",
    "        print(f\"\\n⚠️  ALERT: Columns with >10% null values:\")\n",
    "        for row in high_nulls.iter_rows(named=True):\n",
    "            print(f\"   - {row['column']}: {row['null_percentage']:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n✓ No columns have excessive null values (>10%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28bbdcd",
   "metadata": {
    "papermill": {
     "duration": 0.048837,
     "end_time": "2026-01-01T19:18:25.631243",
     "exception": false,
     "start_time": "2026-01-01T19:18:25.582406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Data Range Validation\n",
    "\n",
    "Check for anomalies in key numeric fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d47dbae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:25.667744Z",
     "iopub.status.busy": "2026-01-01T19:18:25.667114Z",
     "iopub.status.idle": "2026-01-01T19:18:36.954568Z",
     "shell.execute_reply": "2026-01-01T19:18:36.952044Z"
    },
    "papermill": {
     "duration": 11.318559,
     "end_time": "2026-01-01T19:18:36.956496",
     "exception": false,
     "start_time": "2026-01-01T19:18:25.637937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing data ranges...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa4d9d368c94e3690dd181cff381145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Range Summary:\n",
      "shape: (1, 14)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ earliest_ ┆ latest_pi ┆ min_dista ┆ max_dista ┆ … ┆ avg_total ┆ min_passe ┆ max_passe ┆ avg_pass │\n",
      "│ pickup    ┆ ckup      ┆ nce       ┆ nce       ┆   ┆ ---       ┆ ngers     ┆ ngers     ┆ engers   │\n",
      "│ ---       ┆ ---       ┆ ---       ┆ ---       ┆   ┆ f64       ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ datetime[ ┆ datetime[ ┆ f64       ┆ f64       ┆   ┆           ┆ f64       ┆ f64       ┆ f64      │\n",
      "│ μs]       ┆ μs]       ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 2001-01-0 ┆ 2098-09-1 ┆ 0.0       ┆ 398608.62 ┆ … ┆ 25.108223 ┆ 0.0       ┆ 112.0     ┆ 1.364151 │\n",
      "│ 1         ┆ 1         ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 00:03:14  ┆ 02:23:31  ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n",
      "\n",
      "⚠️  ALERT: Potential data anomalies detected:\n",
      "   - Extremely long trip distance: 398608.62 miles\n",
      "   - Negative fare amounts detected\n",
      "   - Extremely high fare: $863372.12\n",
      "   - Negative total amounts detected\n",
      "   - Unusually high passenger count: 112.0\n"
     ]
    }
   ],
   "source": [
    "# Analyze data ranges and potential anomalies\n",
    "if len(data_files) > 0:\n",
    "    print(\"Analyzing data ranges...\\n\")\n",
    "    \n",
    "    pattern_path = str(data_dir / pattern)\n",
    "    \n",
    "    range_query = f\"\"\"\n",
    "    SELECT\n",
    "        MIN(tpep_pickup_datetime) as earliest_pickup,\n",
    "        MAX(tpep_pickup_datetime) as latest_pickup,\n",
    "        MIN(trip_distance) as min_distance,\n",
    "        MAX(trip_distance) as max_distance,\n",
    "        AVG(trip_distance) as avg_distance,\n",
    "        MIN(fare_amount) as min_fare,\n",
    "        MAX(fare_amount) as max_fare,\n",
    "        AVG(fare_amount) as avg_fare,\n",
    "        MIN(total_amount) as min_total,\n",
    "        MAX(total_amount) as max_total,\n",
    "        AVG(total_amount) as avg_total,\n",
    "        MIN(passenger_count) as min_passengers,\n",
    "        MAX(passenger_count) as max_passengers,\n",
    "        AVG(passenger_count) as avg_passengers\n",
    "    FROM '{pattern_path}'\n",
    "    \"\"\"\n",
    "    \n",
    "    ranges = duckdb.sql(range_query).pl()\n",
    "    \n",
    "    print(\"Data Range Summary:\")\n",
    "    print(ranges)\n",
    "    \n",
    "    # Check for potential anomalies\n",
    "    anomalies = []\n",
    "    \n",
    "    row = ranges.row(0, named=True)\n",
    "    \n",
    "    if row['min_distance'] < 0:\n",
    "        anomalies.append(\"Negative trip distances detected\")\n",
    "    if row['max_distance'] > 1000:\n",
    "        anomalies.append(f\"Extremely long trip distance: {row['max_distance']:.2f} miles\")\n",
    "    if row['min_fare'] < 0:\n",
    "        anomalies.append(\"Negative fare amounts detected\")\n",
    "    if row['max_fare'] > 10000:\n",
    "        anomalies.append(f\"Extremely high fare: ${row['max_fare']:.2f}\")\n",
    "    if row['min_total'] < 0:\n",
    "        anomalies.append(\"Negative total amounts detected\")\n",
    "    if row['min_passengers'] < 0:\n",
    "        anomalies.append(\"Negative passenger counts detected\")\n",
    "    if row['max_passengers'] > 9:\n",
    "        anomalies.append(f\"Unusually high passenger count: {row['max_passengers']}\")\n",
    "    \n",
    "    if anomalies:\n",
    "        print(f\"\\n⚠️  ALERT: Potential data anomalies detected:\")\n",
    "        for anomaly in anomalies:\n",
    "            print(f\"   - {anomaly}\")\n",
    "    else:\n",
    "        print(\"\\n✓ No obvious data anomalies detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e20cd0",
   "metadata": {
    "papermill": {
     "duration": 0.006224,
     "end_time": "2026-01-01T19:18:36.969999",
     "exception": false,
     "start_time": "2026-01-01T19:18:36.963775",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Schema Documentation\n",
    "\n",
    "Detailed field descriptions based on NYC TLC data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb1366df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:36.985714Z",
     "iopub.status.busy": "2026-01-01T19:18:36.985092Z",
     "iopub.status.idle": "2026-01-01T19:18:36.995907Z",
     "shell.execute_reply": "2026-01-01T19:18:36.994451Z"
    },
    "papermill": {
     "duration": 0.021496,
     "end_time": "2026-01-01T19:18:36.997240",
     "exception": false,
     "start_time": "2026-01-01T19:18:36.975744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Documentation for NYC Yellow Taxi Trip Records\n",
      "================================================================================\n",
      "\n",
      "Key Fields for Congestion Pricing Analysis:\n",
      "  - PULocationID: Pickup location (taxi zone)\n",
      "  - DOLocationID: Dropoff location (taxi zone)\n",
      "  - tpep_pickup_datetime: Timestamp for time-of-day analysis\n",
      "  - congestion_surcharge: Existing congestion fee (2019+)\n",
      "\n",
      "Note: The congestion pricing zone generally covers Manhattan south of 60th Street.\n",
      "      Taxi zones 1-263 map to specific geographic areas.\n",
      "\n",
      "Full schema saved to outputs/schema_documentation.json\n"
     ]
    }
   ],
   "source": [
    "# NYC Yellow Taxi Trip Record Schema Documentation\n",
    "schema_documentation = {\n",
    "    \"VendorID\": {\n",
    "        \"description\": \"TPEP provider (1=Creative Mobile Technologies, 2=VeriFone Inc.)\",\n",
    "        \"type\": \"Integer\"\n",
    "    },\n",
    "    \"tpep_pickup_datetime\": {\n",
    "        \"description\": \"Date and time when the meter was engaged\",\n",
    "        \"type\": \"Timestamp\"\n",
    "    },\n",
    "    \"tpep_dropoff_datetime\": {\n",
    "        \"description\": \"Date and time when the meter was disengaged\",\n",
    "        \"type\": \"Timestamp\"\n",
    "    },\n",
    "    \"passenger_count\": {\n",
    "        \"description\": \"Number of passengers in the vehicle (driver entered value)\",\n",
    "        \"type\": \"Integer\"\n",
    "    },\n",
    "    \"trip_distance\": {\n",
    "        \"description\": \"Trip distance in miles reported by the taximeter\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"RatecodeID\": {\n",
    "        \"description\": \"Rate code (1=Standard, 2=JFK, 3=Newark, 4=Nassau/Westchester, 5=Negotiated, 6=Group ride)\",\n",
    "        \"type\": \"Integer\"\n",
    "    },\n",
    "    \"store_and_fwd_flag\": {\n",
    "        \"description\": \"Trip record held in vehicle memory before sending (Y=store and forward, N=not)\",\n",
    "        \"type\": \"String\"\n",
    "    },\n",
    "    \"PULocationID\": {\n",
    "        \"description\": \"TLC Taxi Zone where the meter was engaged\",\n",
    "        \"type\": \"Integer\",\n",
    "        \"note\": \"Critical for congestion pricing analysis\"\n",
    "    },\n",
    "    \"DOLocationID\": {\n",
    "        \"description\": \"TLC Taxi Zone where the meter was disengaged\",\n",
    "        \"type\": \"Integer\",\n",
    "        \"note\": \"Critical for congestion pricing analysis\"\n",
    "    },\n",
    "    \"payment_type\": {\n",
    "        \"description\": \"Payment method (1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided)\",\n",
    "        \"type\": \"Integer\"\n",
    "    },\n",
    "    \"fare_amount\": {\n",
    "        \"description\": \"Time-and-distance fare calculated by the meter\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"extra\": {\n",
    "        \"description\": \"Miscellaneous extras and surcharges (rush hour, overnight)\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"mta_tax\": {\n",
    "        \"description\": \"$0.50 MTA tax automatically triggered based on metered rate\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"tip_amount\": {\n",
    "        \"description\": \"Tip amount (automatically populated for credit card, cash tips not included)\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"tolls_amount\": {\n",
    "        \"description\": \"Total amount of all tolls paid in trip\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"improvement_surcharge\": {\n",
    "        \"description\": \"$0.30 improvement surcharge assessed on hailed trips\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"total_amount\": {\n",
    "        \"description\": \"Total amount charged to passengers (does not include cash tips)\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"congestion_surcharge\": {\n",
    "        \"description\": \"Congestion surcharge for trips in Manhattan south of 96th Street\",\n",
    "        \"type\": \"Float\",\n",
    "        \"note\": \"Introduced in 2019 - may not be present in older files\"\n",
    "    },\n",
    "    \"airport_fee\": {\n",
    "        \"description\": \"$1.25 fee for pickups at LaGuardia and JFK airports\",\n",
    "        \"type\": \"Float\",\n",
    "        \"note\": \"Added in more recent data - may not be present in older files\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Schema Documentation for NYC Yellow Taxi Trip Records\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Fields for Congestion Pricing Analysis:\")\n",
    "print(\"  - PULocationID: Pickup location (taxi zone)\")\n",
    "print(\"  - DOLocationID: Dropoff location (taxi zone)\")\n",
    "print(\"  - tpep_pickup_datetime: Timestamp for time-of-day analysis\")\n",
    "print(\"  - congestion_surcharge: Existing congestion fee (2019+)\")\n",
    "print(\"\\nNote: The congestion pricing zone generally covers Manhattan south of 60th Street.\")\n",
    "print(\"      Taxi zones 1-263 map to specific geographic areas.\")\n",
    "print(\"\\nFull schema saved to outputs/schema_documentation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1eaa3c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:37.014307Z",
     "iopub.status.busy": "2026-01-01T19:18:37.013017Z",
     "iopub.status.idle": "2026-01-01T19:18:37.027186Z",
     "shell.execute_reply": "2026-01-01T19:18:37.024025Z"
    },
    "papermill": {
     "duration": 0.024878,
     "end_time": "2026-01-01T19:18:37.029220",
     "exception": false,
     "start_time": "2026-01-01T19:18:37.004342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Schema documentation saved\n"
     ]
    }
   ],
   "source": [
    "# Save schema documentation to outputs\n",
    "output_dir = Path(\"outputs\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "schema_output = {\n",
    "    \"schema\": schema_documentation,\n",
    "    \"columns\": schema_columns if 'schema_columns' in dir() else [],\n",
    "    \"data_types\": [str(t) for t in schema_types] if 'schema_types' in dir() else [],\n",
    "    \"generated_at\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "with open(output_dir / \"schema_documentation.json\", \"w\") as f:\n",
    "    json.dump(schema_output, f, indent=2)\n",
    "\n",
    "print(\"✓ Schema documentation saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3981e313",
   "metadata": {
    "papermill": {
     "duration": 0.006334,
     "end_time": "2026-01-01T19:18:37.042410",
     "exception": false,
     "start_time": "2026-01-01T19:18:37.036076",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7. Validation Summary\n",
    "\n",
    "Overall data quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba721f4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-01T19:18:37.065986Z",
     "iopub.status.busy": "2026-01-01T19:18:37.065021Z",
     "iopub.status.idle": "2026-01-01T19:18:37.075307Z",
     "shell.execute_reply": "2026-01-01T19:18:37.074310Z"
    },
    "papermill": {
     "duration": 0.020941,
     "end_time": "2026-01-01T19:18:37.076818",
     "exception": false,
     "start_time": "2026-01-01T19:18:37.055877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "VALIDATION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "✓ Dataset Status: READY FOR ANALYSIS\n",
      "\n",
      "Dataset Overview:\n",
      "  - Files: 59\n",
      "  - Total Records: 194,457,948\n",
      "  - Date Range: 2021-01 to 2025-11\n",
      "  - Total Size: 3032.40 MB\n",
      "\n",
      "Data Quality:\n",
      "  ⚠️  Issues Found: 34 file(s) with schema issues, 5 data anomaly/anomalies\n",
      "  ℹ️  Review alerts above for details\n",
      "\n",
      "Next Steps:\n",
      "  1. Review any alerts or warnings above\n",
      "  2. Use schema documentation in outputs/schema_documentation.json\n",
      "  3. Proceed with analysis in other notebooks\n",
      "  4. Reference taxi zones at: https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(data_files) > 0:\n",
    "    print(f\"\\n✓ Dataset Status: READY FOR ANALYSIS\")\n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  - Files: {len(data_files)}\")\n",
    "    if 'stats_df' in dir() and len(stats_df) > 0:\n",
    "        print(f\"  - Total Records: {stats_df['row_count'].sum():,}\")\n",
    "        print(f\"  - Date Range: {files_df['year_month'].min()} to {files_df['year_month'].max()}\")\n",
    "        print(f\"  - Total Size: {stats_df['size_mb'].sum():.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nData Quality:\")\n",
    "    issues = []\n",
    "    if 'missing_months' in dir() and len(missing_months) > 0:\n",
    "        issues.append(f\"{len(missing_months)} missing month(s)\")\n",
    "    if 'inconsistent_files' in dir() and len(inconsistent_files) > 0:\n",
    "        issues.append(f\"{len(inconsistent_files)} file(s) with schema issues\")\n",
    "    if 'anomalies' in dir() and len(anomalies) > 0:\n",
    "        issues.append(f\"{len(anomalies)} data anomaly/anomalies\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"  ⚠️  Issues Found: {', '.join(issues)}\")\n",
    "        print(f\"  ℹ️  Review alerts above for details\")\n",
    "    else:\n",
    "        print(f\"  ✓ No critical issues detected\")\n",
    "    \n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(f\"  1. Review any alerts or warnings above\")\n",
    "    print(f\"  2. Use schema documentation in outputs/schema_documentation.json\")\n",
    "    print(f\"  3. Proceed with analysis in other notebooks\")\n",
    "    print(f\"  4. Reference taxi zones at: https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Dataset Status: NO DATA FOUND\")\n",
    "    print(f\"\\nPlease run the data download notebook first to fetch NYC taxi data.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.535314,
   "end_time": "2026-01-01T19:18:37.628838",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/data-validation.ipynb",
   "output_path": "out/data-validation.ipynb",
   "parameters": {},
   "start_time": "2026-01-01T19:18:07.093524",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "082ca7da00374a82a026655c27918475": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "auto"
      }
     },
     "3aa4d9d368c94e3690dd181cff381145": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_082ca7da00374a82a026655c27918475",
       "max": 100.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6691ad397ee74b03ab23eab644743128",
       "tabbable": null,
       "tooltip": null,
       "value": 100.0
      }
     },
     "6691ad397ee74b03ab23eab644743128": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": "black",
       "description_width": ""
      }
     },
     "7ca82e8b479a4b6c8a9868c62caead61": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c7854387e8b34ca59604cbb7022f138b",
       "max": 100.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f7a5c7bed8a84439aa7b08bb018760bd",
       "tabbable": null,
       "tooltip": null,
       "value": 100.0
      }
     },
     "c7854387e8b34ca59604cbb7022f138b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": "auto"
      }
     },
     "f7a5c7bed8a84439aa7b08bb018760bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": "black",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}