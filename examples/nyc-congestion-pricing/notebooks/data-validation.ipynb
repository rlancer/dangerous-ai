{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation Report\n",
    "\n",
    "This notebook validates the NYC Yellow Taxi trip data and provides:\n",
    "- **Completeness Check**: Identifies missing months in the dataset\n",
    "- **Schema Validation**: Ensures consistency across all Parquet files\n",
    "- **Data Quality Metrics**: Row counts, null values, and data ranges\n",
    "- **Schema Documentation**: Detailed field descriptions for use in other notebooks\n",
    "\n",
    "Run this notebook before performing analysis to ensure data integrity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Inventory\n",
    "\n",
    "Scan the data directory and catalog all available files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\ndata_dir = Path(\"data\")\ndata_type = \"yellow\"\npattern = f\"{data_type}_tripdata_*.parquet\"\n\n# Find all data files\ndata_files = sorted(data_dir.glob(pattern))\n\nprint(f\"Data Directory: {data_dir.absolute()}\")\nprint(f\"Pattern: {pattern}\")\nprint(f\"Files Found: {len(data_files)}\")\nprint(\"\\n\" + \"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract file metadata\nfile_info = []\nfor file_path in data_files:\n    # Parse year-month from filename\n    # Format: yellow_tripdata_YYYY-MM.parquet\n    parts = file_path.stem.split(\"_\")\n    if len(parts) >= 3:\n        year_month = parts[2]  # e.g., \"2021-01\"\n        year, month = map(int, year_month.split(\"-\"))\n        \n        file_info.append({\n            \"filename\": file_path.name,\n            \"year\": year,\n            \"month\": month,\n            \"year_month\": year_month,\n            \"size_mb\": file_path.stat().st_size / (1024 * 1024),\n            \"path\": str(file_path)\n        })\n\n# Convert to Polars DataFrame for easy analysis\nif len(file_info) > 0:\n    files_df = pl.DataFrame(file_info).sort([\"year\", \"month\"])\n    print(f\"\\nFile Inventory ({len(files_df)} files):\")\n    print(files_df.select([\"year_month\", \"size_mb\"]))\nelse:\n    files_df = pl.DataFrame()\n    print(\"\\nNo valid data files found\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Completeness Check\n",
    "\n",
    "Identify missing months in the expected date range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(files_df) > 0:\n",
    "    # Determine expected date range\n",
    "    min_year = files_df[\"year\"].min()\n",
    "    min_month = files_df.filter(pl.col(\"year\") == min_year)[\"month\"].min()\n",
    "    max_year = files_df[\"year\"].max()\n",
    "    max_month = files_df.filter(pl.col(\"year\") == max_year)[\"month\"].max()\n",
    "    \n",
    "    print(f\"Date Range: {min_year}-{min_month:02d} to {max_year}-{max_month:02d}\")\n",
    "    \n",
    "    # Generate expected months\n",
    "    start_date = datetime(min_year, min_month, 1)\n",
    "    end_date = datetime(max_year, max_month, 1)\n",
    "    \n",
    "    expected_months = []\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        expected_months.append(f\"{current.year}-{current.month:02d}\")\n",
    "        # Move to next month\n",
    "        if current.month == 12:\n",
    "            current = current.replace(year=current.year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=current.month + 1)\n",
    "    \n",
    "    # Find missing months\n",
    "    available_months = set(files_df[\"year_month\"].to_list())\n",
    "    missing_months = [m for m in expected_months if m not in available_months]\n",
    "    \n",
    "    print(f\"\\nExpected Months: {len(expected_months)}\")\n",
    "    print(f\"Available Months: {len(available_months)}\")\n",
    "    print(f\"Missing Months: {len(missing_months)}\")\n",
    "    \n",
    "    if missing_months:\n",
    "        print(f\"\\n⚠️  ALERT: Missing data for the following months:\")\n",
    "        for month in missing_months:\n",
    "            print(f\"   - {month}\")\n",
    "    else:\n",
    "        print(\"\\n✓ All expected months are present\")\n",
    "else:\n",
    "    print(\"⚠️  No data files found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Schema Validation\n",
    "\n",
    "Verify that all files have consistent schemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema from the first file as reference\n",
    "if len(data_files) > 0:\n",
    "    reference_file = data_files[0]\n",
    "    \n",
    "    # Use DuckDB to get schema information\n",
    "    reference_schema = duckdb.sql(f\"\"\"\n",
    "        SELECT * FROM '{reference_file}' LIMIT 0\n",
    "    \"\"\").description\n",
    "    \n",
    "    schema_columns = [col[0] for col in reference_schema]\n",
    "    schema_types = [col[1] for col in reference_schema]\n",
    "    \n",
    "    print(f\"Reference Schema (from {reference_file.name}):\")\n",
    "    print(f\"Total Columns: {len(schema_columns)}\\n\")\n",
    "    \n",
    "    # Display schema\n",
    "    schema_df = pl.DataFrame({\n",
    "        \"column_name\": schema_columns,\n",
    "        \"data_type\": [str(t) for t in schema_types]\n",
    "    })\n",
    "    print(schema_df)\n",
    "else:\n",
    "    print(\"No files to analyze\")\n",
    "    schema_columns = []\n",
    "    schema_types = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check schema consistency across all files\n",
    "if len(data_files) > 1:\n",
    "    print(\"\\nChecking schema consistency across all files...\\n\")\n",
    "    \n",
    "    inconsistent_files = []\n",
    "    \n",
    "    for file_path in data_files[1:]:\n",
    "        file_schema = duckdb.sql(f\"\"\"\n",
    "            SELECT * FROM '{file_path}' LIMIT 0\n",
    "        \"\"\").description\n",
    "        \n",
    "        file_columns = [col[0] for col in file_schema]\n",
    "        file_types = [str(col[1]) for col in file_schema]\n",
    "        \n",
    "        # Check for differences\n",
    "        if file_columns != schema_columns:\n",
    "            inconsistent_files.append({\n",
    "                \"file\": file_path.name,\n",
    "                \"issue\": \"Column names mismatch\",\n",
    "                \"details\": f\"Expected {len(schema_columns)} columns, got {len(file_columns)}\"\n",
    "            })\n",
    "        elif [str(t) for t in schema_types] != file_types:\n",
    "            inconsistent_files.append({\n",
    "                \"file\": file_path.name,\n",
    "                \"issue\": \"Data types mismatch\",\n",
    "                \"details\": \"Column types differ from reference\"\n",
    "            })\n",
    "    \n",
    "    if inconsistent_files:\n",
    "        print(f\"⚠️  ALERT: Found {len(inconsistent_files)} files with schema inconsistencies:\\n\")\n",
    "        for issue in inconsistent_files:\n",
    "            print(f\"   - {issue['file']}\")\n",
    "            print(f\"     {issue['issue']}: {issue['details']}\")\n",
    "    else:\n",
    "        print(\"✓ All files have consistent schemas\")\n",
    "elif len(data_files) == 1:\n",
    "    print(\"Only one file available - cannot check consistency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Metrics\n",
    "\n",
    "Analyze row counts, null values, and data ranges for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get row counts and basic statistics for each file\n",
    "if len(data_files) > 0:\n",
    "    print(\"Analyzing data quality metrics...\\n\")\n",
    "    \n",
    "    file_stats = []\n",
    "    \n",
    "    for file_path in data_files:\n",
    "        # Get row count\n",
    "        row_count = duckdb.sql(f\"\"\"\n",
    "            SELECT COUNT(*) as cnt FROM '{file_path}'\n",
    "        \"\"\").fetchone()[0]\n",
    "        \n",
    "        # Extract year-month\n",
    "        year_month = file_path.stem.split(\"_\")[2]\n",
    "        \n",
    "        file_stats.append({\n",
    "            \"year_month\": year_month,\n",
    "            \"filename\": file_path.name,\n",
    "            \"row_count\": row_count,\n",
    "            \"size_mb\": file_path.stat().st_size / (1024 * 1024)\n",
    "        })\n",
    "    \n",
    "    stats_df = pl.DataFrame(file_stats).sort(\"year_month\")\n",
    "    \n",
    "    print(\"File Statistics:\")\n",
    "    print(stats_df.select([\"year_month\", \"row_count\", \"size_mb\"]))\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(\"Summary:\")\n",
    "    print(f\"  Total Files: {len(stats_df)}\")\n",
    "    print(f\"  Total Rows: {stats_df['row_count'].sum():,}\")\n",
    "    print(f\"  Total Size: {stats_df['size_mb'].sum():.2f} MB\")\n",
    "    print(f\"  Avg Rows/File: {stats_df['row_count'].mean():,.0f}\")\n",
    "    print(f\"  Min Rows: {stats_df['row_count'].min():,} ({stats_df.filter(pl.col('row_count') == pl.col('row_count').min())['year_month'][0]})\")\n",
    "    print(f\"  Max Rows: {stats_df['row_count'].max():,} ({stats_df.filter(pl.col('row_count') == pl.col('row_count').max())['year_month'][0]})\")\n",
    "else:\n",
    "    print(\"No files to analyze\")\n",
    "    stats_df = pl.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in key columns\n",
    "if len(data_files) > 0:\n",
    "    print(\"\\nChecking for null values in key columns...\\n\")\n",
    "    \n",
    "    # Query all files at once using glob pattern\n",
    "    pattern_path = str(data_dir / pattern)\n",
    "    \n",
    "    # Key columns to check\n",
    "    key_columns = [\n",
    "        \"VendorID\",\n",
    "        \"tpep_pickup_datetime\",\n",
    "        \"tpep_dropoff_datetime\",\n",
    "        \"passenger_count\",\n",
    "        \"trip_distance\",\n",
    "        \"PULocationID\",\n",
    "        \"DOLocationID\",\n",
    "        \"payment_type\",\n",
    "        \"fare_amount\",\n",
    "        \"total_amount\"\n",
    "    ]\n",
    "    \n",
    "    # Build dynamic query to count nulls\n",
    "    null_checks = []\n",
    "    for col in key_columns:\n",
    "        null_checks.append(f\"SUM(CASE WHEN {col} IS NULL THEN 1 ELSE 0 END) as {col}_nulls\")\n",
    "    \n",
    "    null_query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        {', '.join(null_checks)}\n",
    "    FROM '{pattern_path}'\n",
    "    \"\"\"\n",
    "    \n",
    "    null_results = duckdb.sql(null_query).pl()\n",
    "    \n",
    "    # Calculate null percentages\n",
    "    total_rows = null_results[\"total_rows\"][0]\n",
    "    print(f\"Total rows across all files: {total_rows:,}\\n\")\n",
    "    \n",
    "    null_summary = []\n",
    "    for col in key_columns:\n",
    "        null_count = null_results[f\"{col}_nulls\"][0]\n",
    "        null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "        null_summary.append({\n",
    "            \"column\": col,\n",
    "            \"null_count\": null_count,\n",
    "            \"null_percentage\": null_pct\n",
    "        })\n",
    "    \n",
    "    null_df = pl.DataFrame(null_summary).sort(\"null_percentage\", descending=True)\n",
    "    print(\"Null Value Analysis:\")\n",
    "    print(null_df)\n",
    "    \n",
    "    # Alert on high null percentages\n",
    "    high_nulls = null_df.filter(pl.col(\"null_percentage\") > 10)\n",
    "    if len(high_nulls) > 0:\n",
    "        print(f\"\\n⚠️  ALERT: Columns with >10% null values:\")\n",
    "        for row in high_nulls.iter_rows(named=True):\n",
    "            print(f\"   - {row['column']}: {row['null_percentage']:.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n✓ No columns have excessive null values (>10%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Range Validation\n",
    "\n",
    "Check for anomalies in key numeric fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze data ranges and potential anomalies\n",
    "if len(data_files) > 0:\n",
    "    print(\"Analyzing data ranges...\\n\")\n",
    "    \n",
    "    pattern_path = str(data_dir / pattern)\n",
    "    \n",
    "    range_query = f\"\"\"\n",
    "    SELECT\n",
    "        MIN(tpep_pickup_datetime) as earliest_pickup,\n",
    "        MAX(tpep_pickup_datetime) as latest_pickup,\n",
    "        MIN(trip_distance) as min_distance,\n",
    "        MAX(trip_distance) as max_distance,\n",
    "        AVG(trip_distance) as avg_distance,\n",
    "        MIN(fare_amount) as min_fare,\n",
    "        MAX(fare_amount) as max_fare,\n",
    "        AVG(fare_amount) as avg_fare,\n",
    "        MIN(total_amount) as min_total,\n",
    "        MAX(total_amount) as max_total,\n",
    "        AVG(total_amount) as avg_total,\n",
    "        MIN(passenger_count) as min_passengers,\n",
    "        MAX(passenger_count) as max_passengers,\n",
    "        AVG(passenger_count) as avg_passengers\n",
    "    FROM '{pattern_path}'\n",
    "    \"\"\"\n",
    "    \n",
    "    ranges = duckdb.sql(range_query).pl()\n",
    "    \n",
    "    print(\"Data Range Summary:\")\n",
    "    print(ranges)\n",
    "    \n",
    "    # Check for potential anomalies\n",
    "    anomalies = []\n",
    "    \n",
    "    row = ranges.row(0, named=True)\n",
    "    \n",
    "    if row['min_distance'] < 0:\n",
    "        anomalies.append(\"Negative trip distances detected\")\n",
    "    if row['max_distance'] > 1000:\n",
    "        anomalies.append(f\"Extremely long trip distance: {row['max_distance']:.2f} miles\")\n",
    "    if row['min_fare'] < 0:\n",
    "        anomalies.append(\"Negative fare amounts detected\")\n",
    "    if row['max_fare'] > 10000:\n",
    "        anomalies.append(f\"Extremely high fare: ${row['max_fare']:.2f}\")\n",
    "    if row['min_total'] < 0:\n",
    "        anomalies.append(\"Negative total amounts detected\")\n",
    "    if row['min_passengers'] < 0:\n",
    "        anomalies.append(\"Negative passenger counts detected\")\n",
    "    if row['max_passengers'] > 9:\n",
    "        anomalies.append(f\"Unusually high passenger count: {row['max_passengers']}\")\n",
    "    \n",
    "    if anomalies:\n",
    "        print(f\"\\n⚠️  ALERT: Potential data anomalies detected:\")\n",
    "        for anomaly in anomalies:\n",
    "            print(f\"   - {anomaly}\")\n",
    "    else:\n",
    "        print(\"\\n✓ No obvious data anomalies detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Schema Documentation\n",
    "\n",
    "Detailed field descriptions based on NYC TLC data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYC Yellow Taxi Trip Record Schema Documentation\n",
    "schema_documentation = {\n",
    "    \"VendorID\": {\n",
    "        \"description\": \"TPEP provider (1=Creative Mobile Technologies, 2=VeriFone Inc.)\",\n",
    "        \"type\": \"Integer\"\n",
    "    },\n",
    "    \"tpep_pickup_datetime\": {\n",
    "        \"description\": \"Date and time when the meter was engaged\",\n",
    "        \"type\": \"Timestamp\"\n",
    "    },\n",
    "    \"tpep_dropoff_datetime\": {\n",
    "        \"description\": \"Date and time when the meter was disengaged\",\n",
    "        \"type\": \"Timestamp\"\n",
    "    },\n",
    "    \"passenger_count\": {\n",
    "        \"description\": \"Number of passengers in the vehicle (driver entered value)\",\n",
    "        \"type\": \"Integer\"\n",
    "    },\n",
    "    \"trip_distance\": {\n",
    "        \"description\": \"Trip distance in miles reported by the taximeter\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"RatecodeID\": {\n",
    "        \"description\": \"Rate code (1=Standard, 2=JFK, 3=Newark, 4=Nassau/Westchester, 5=Negotiated, 6=Group ride)\",\n",
    "        \"type\": \"Integer\"\n",
    "    },\n",
    "    \"store_and_fwd_flag\": {\n",
    "        \"description\": \"Trip record held in vehicle memory before sending (Y=store and forward, N=not)\",\n",
    "        \"type\": \"String\"\n",
    "    },\n",
    "    \"PULocationID\": {\n",
    "        \"description\": \"TLC Taxi Zone where the meter was engaged\",\n",
    "        \"type\": \"Integer\",\n",
    "        \"note\": \"Critical for congestion pricing analysis\"\n",
    "    },\n",
    "    \"DOLocationID\": {\n",
    "        \"description\": \"TLC Taxi Zone where the meter was disengaged\",\n",
    "        \"type\": \"Integer\",\n",
    "        \"note\": \"Critical for congestion pricing analysis\"\n",
    "    },\n",
    "    \"payment_type\": {\n",
    "        \"description\": \"Payment method (1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided)\",\n",
    "        \"type\": \"Integer\"\n",
    "    },\n",
    "    \"fare_amount\": {\n",
    "        \"description\": \"Time-and-distance fare calculated by the meter\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"extra\": {\n",
    "        \"description\": \"Miscellaneous extras and surcharges (rush hour, overnight)\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"mta_tax\": {\n",
    "        \"description\": \"$0.50 MTA tax automatically triggered based on metered rate\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"tip_amount\": {\n",
    "        \"description\": \"Tip amount (automatically populated for credit card, cash tips not included)\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"tolls_amount\": {\n",
    "        \"description\": \"Total amount of all tolls paid in trip\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"improvement_surcharge\": {\n",
    "        \"description\": \"$0.30 improvement surcharge assessed on hailed trips\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"total_amount\": {\n",
    "        \"description\": \"Total amount charged to passengers (does not include cash tips)\",\n",
    "        \"type\": \"Float\"\n",
    "    },\n",
    "    \"congestion_surcharge\": {\n",
    "        \"description\": \"Congestion surcharge for trips in Manhattan south of 96th Street\",\n",
    "        \"type\": \"Float\",\n",
    "        \"note\": \"Introduced in 2019 - may not be present in older files\"\n",
    "    },\n",
    "    \"airport_fee\": {\n",
    "        \"description\": \"$1.25 fee for pickups at LaGuardia and JFK airports\",\n",
    "        \"type\": \"Float\",\n",
    "        \"note\": \"Added in more recent data - may not be present in older files\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Schema Documentation for NYC Yellow Taxi Trip Records\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nKey Fields for Congestion Pricing Analysis:\")\n",
    "print(\"  - PULocationID: Pickup location (taxi zone)\")\n",
    "print(\"  - DOLocationID: Dropoff location (taxi zone)\")\n",
    "print(\"  - tpep_pickup_datetime: Timestamp for time-of-day analysis\")\n",
    "print(\"  - congestion_surcharge: Existing congestion fee (2019+)\")\n",
    "print(\"\\nNote: The congestion pricing zone generally covers Manhattan south of 60th Street.\")\n",
    "print(\"      Taxi zones 1-263 map to specific geographic areas.\")\n",
    "print(\"\\nFull schema saved to outputs/schema_documentation.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Save schema documentation to outputs\noutput_dir = Path(\"outputs\")\noutput_dir.mkdir(parents=True, exist_ok=True)\n\nschema_output = {\n    \"schema\": schema_documentation,\n    \"columns\": schema_columns if 'schema_columns' in dir() else [],\n    \"data_types\": [str(t) for t in schema_types] if 'schema_types' in dir() else [],\n    \"generated_at\": datetime.now().isoformat()\n}\n\nwith open(output_dir / \"schema_documentation.json\", \"w\") as f:\n    json.dump(schema_output, f, indent=2)\n\nprint(\"✓ Schema documentation saved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation Summary\n",
    "\n",
    "Overall data quality assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDATION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(data_files) > 0:\n",
    "    print(f\"\\n✓ Dataset Status: READY FOR ANALYSIS\")\n",
    "    print(f\"\\nDataset Overview:\")\n",
    "    print(f\"  - Files: {len(data_files)}\")\n",
    "    if 'stats_df' in dir() and len(stats_df) > 0:\n",
    "        print(f\"  - Total Records: {stats_df['row_count'].sum():,}\")\n",
    "        print(f\"  - Date Range: {files_df['year_month'].min()} to {files_df['year_month'].max()}\")\n",
    "        print(f\"  - Total Size: {stats_df['size_mb'].sum():.2f} MB\")\n",
    "    \n",
    "    print(f\"\\nData Quality:\")\n",
    "    issues = []\n",
    "    if 'missing_months' in dir() and len(missing_months) > 0:\n",
    "        issues.append(f\"{len(missing_months)} missing month(s)\")\n",
    "    if 'inconsistent_files' in dir() and len(inconsistent_files) > 0:\n",
    "        issues.append(f\"{len(inconsistent_files)} file(s) with schema issues\")\n",
    "    if 'anomalies' in dir() and len(anomalies) > 0:\n",
    "        issues.append(f\"{len(anomalies)} data anomaly/anomalies\")\n",
    "    \n",
    "    if issues:\n",
    "        print(f\"  ⚠️  Issues Found: {', '.join(issues)}\")\n",
    "        print(f\"  ℹ️  Review alerts above for details\")\n",
    "    else:\n",
    "        print(f\"  ✓ No critical issues detected\")\n",
    "    \n",
    "    print(f\"\\nNext Steps:\")\n",
    "    print(f\"  1. Review any alerts or warnings above\")\n",
    "    print(f\"  2. Use schema documentation in outputs/schema_documentation.json\")\n",
    "    print(f\"  3. Proceed with analysis in other notebooks\")\n",
    "    print(f\"  4. Reference taxi zones at: https://data.cityofnewyork.us/Transportation/NYC-Taxi-Zones/d3c5-ddgc\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Dataset Status: NO DATA FOUND\")\n",
    "    print(f\"\\nPlease run the data download notebook first to fetch NYC taxi data.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}