{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NYC Congestion Pricing Analysis\n",
    "\n",
    "This notebook downloads and analyzes NYC TLC (Taxi & Limousine Commission) trip record data.\n",
    "\n",
    "## Data Source\n",
    "\n",
    "NYC publishes taxi trip data at: https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "The data includes:\n",
    "- **Yellow Taxi**: Traditional yellow cabs\n",
    "- **Green Taxi**: Street hail livery vehicles\n",
    "- **For-Hire Vehicle (FHV)**: Standard FHV services\n",
    "- **High Volume FHV (HVFHV)**: Rideshare services\n",
    "\n",
    "All files are in Parquet format with the naming pattern:\n",
    "`https://d37ci6vzurychx.cloudfront.net/trip-data/{type}_tripdata_{year}-{month}.parquet`\n",
    "\n",
    "## Example Prompt for Downloading Data\n",
    "\n",
    "To download the data, you can use this prompt with Claude Code in the project directory:\n",
    "\n",
    "```\n",
    "Download the last 5 years of NYC Yellow Taxi trip data in Parquet format \n",
    "from the NYC TLC website. Use Playwright to parse the page and get the \n",
    "most recent available data files. Save them to the data/ directory.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "tags": [
     "parameters"
    ],
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Parameters (tagged for papermill)\n",
    "data_type = \"yellow\"  # Options: yellow, green, fhv, fhvhv\n",
    "years_back = 5  # Number of years of historical data to download\n",
    "data_dir = \"data\"\n",
    "output_path = \"outputs/result.parquet\"\n",
    "use_playwright = False  # Set to True to dynamically parse the TLC website"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from urllib.request import urlretrieve\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory: c:\\Users\\Rob\\Desktop\\ai-for-the-rest\\examples\\nyc-congestion-pricing\\notebooks\\data\n"
     ]
    }
   ],
   "source": [
    "# Create data directory if it doesn't exist\n",
    "Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Data directory: {Path(data_dir).absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download NYC TLC Data\n",
    "\n",
    "Two approaches are provided:\n",
    "\n",
    "1. **Direct Download** (default): Uses the known URL pattern to download files\n",
    "2. **Playwright-based** (optional): Dynamically parses the TLC website to get the latest available files\n",
    "\n",
    "The direct approach is faster and doesn't require browser automation. Use Playwright if you need to ensure you're getting the absolute latest data or if the URL pattern changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct download function defined\n"
     ]
    }
   ],
   "source": [
    "def download_tlc_data_direct(data_type, years_back, data_dir):\n",
    "    \"\"\"\n",
    "    Download NYC TLC data using direct URL pattern.\n",
    "    \n",
    "    Args:\n",
    "        data_type: Type of data (yellow, green, fhv, fhvhv)\n",
    "        years_back: Number of years of historical data\n",
    "        data_dir: Directory to save files\n",
    "    \n",
    "    Returns:\n",
    "        List of downloaded file paths\n",
    "    \"\"\"\n",
    "    base_url = \"https://d37ci6vzurychx.cloudfront.net/trip-data\"\n",
    "    downloaded_files = []\n",
    "    \n",
    "    # Calculate date range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=365 * years_back)\n",
    "    \n",
    "    # Generate list of year-month combinations\n",
    "    current = start_date.replace(day=1)\n",
    "    while current <= end_date:\n",
    "        year = current.year\n",
    "        month = current.month\n",
    "        \n",
    "        # Construct filename and URL\n",
    "        filename = f\"{data_type}_tripdata_{year}-{month:02d}.parquet\"\n",
    "        url = f\"{base_url}/{filename}\"\n",
    "        local_path = Path(data_dir) / filename\n",
    "        \n",
    "        # Download if not already present\n",
    "        if not local_path.exists():\n",
    "            try:\n",
    "                print(f\"Downloading {filename}...\")\n",
    "                urlretrieve(url, local_path)\n",
    "                downloaded_files.append(str(local_path))\n",
    "                print(f\"  ✓ Saved to {local_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Failed to download {filename}: {e}\")\n",
    "        else:\n",
    "            print(f\"  ⊙ {filename} already exists, skipping\")\n",
    "            downloaded_files.append(str(local_path))\n",
    "        \n",
    "        # Move to next month\n",
    "        if month == 12:\n",
    "            current = current.replace(year=year + 1, month=1)\n",
    "        else:\n",
    "            current = current.replace(month=month + 1)\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "print(\"Direct download function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playwright download function defined\n"
     ]
    }
   ],
   "source": [
    "def download_tlc_data_playwright(data_type, years_back, data_dir):\n",
    "    \"\"\"\n",
    "    Download NYC TLC data by parsing the official website with Playwright.\n",
    "    This ensures you get the latest available data even if URL patterns change.\n",
    "    \n",
    "    Requires: playwright (install with: playwright install)\n",
    "    \n",
    "    Args:\n",
    "        data_type: Type of data (yellow, green, fhv, fhvhv)\n",
    "        years_back: Number of years of historical data\n",
    "        data_dir: Directory to save files\n",
    "    \n",
    "    Returns:\n",
    "        List of downloaded file paths\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from playwright.sync_api import sync_playwright\n",
    "    except ImportError:\n",
    "        print(\"Error: Playwright not installed. Install with: pip install playwright && playwright install\")\n",
    "        return []\n",
    "    \n",
    "    downloaded_files = []\n",
    "    \n",
    "    # Calculate cutoff date\n",
    "    cutoff_date = datetime.now() - timedelta(days=365 * years_back)\n",
    "    \n",
    "    with sync_playwright() as p:\n",
    "        browser = p.chromium.launch(headless=True)\n",
    "        page = browser.new_page()\n",
    "        \n",
    "        print(\"Loading NYC TLC trip record data page...\")\n",
    "        page.goto(\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\")\n",
    "        page.wait_for_load_state(\"networkidle\")\n",
    "        \n",
    "        # Find all parquet download links\n",
    "        print(f\"Parsing page for {data_type} parquet files...\")\n",
    "        links = page.query_selector_all(\"a[href*='.parquet']\")\n",
    "        \n",
    "        for link in links:\n",
    "            href = link.get_attribute(\"href\")\n",
    "            if not href or data_type not in href:\n",
    "                continue\n",
    "            \n",
    "            # Extract year and month from filename\n",
    "            # Expected format: {type}_tripdata_{year}-{month}.parquet\n",
    "            try:\n",
    "                parts = href.split(\"/\")[-1].replace(\".parquet\", \"\").split(\"_\")\n",
    "                if len(parts) >= 3:\n",
    "                    year_month = parts[2]  # e.g., \"2025-01\"\n",
    "                    year, month = map(int, year_month.split(\"-\"))\n",
    "                    file_date = datetime(year, month, 1)\n",
    "                    \n",
    "                    # Skip if before cutoff date\n",
    "                    if file_date < cutoff_date:\n",
    "                        continue\n",
    "                    \n",
    "                    filename = href.split(\"/\")[-1]\n",
    "                    local_path = Path(data_dir) / filename\n",
    "                    \n",
    "                    # Download if not already present\n",
    "                    if not local_path.exists():\n",
    "                        print(f\"Downloading {filename}...\")\n",
    "                        full_url = href if href.startswith(\"http\") else f\"https://d37ci6vzurychx.cloudfront.net{href}\"\n",
    "                        urlretrieve(full_url, local_path)\n",
    "                        downloaded_files.append(str(local_path))\n",
    "                        print(f\"  ✓ Saved to {local_path}\")\n",
    "                    else:\n",
    "                        print(f\"  ⊙ {filename} already exists, skipping\")\n",
    "                        downloaded_files.append(str(local_path))\n",
    "            except Exception as e:\n",
    "                print(f\"  ✗ Error processing {href}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        browser.close()\n",
    "    \n",
    "    return downloaded_files\n",
    "\n",
    "print(\"Playwright download function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting download: yellow taxi data for the last 5 years\n",
      "Method: Direct (URL pattern)\n",
      "------------------------------------------------------------\n",
      "Downloading yellow_tripdata_2020-12.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2020-12.parquet\n",
      "Downloading yellow_tripdata_2021-01.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-01.parquet\n",
      "Downloading yellow_tripdata_2021-02.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-02.parquet\n",
      "Downloading yellow_tripdata_2021-03.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-03.parquet\n",
      "Downloading yellow_tripdata_2021-04.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-04.parquet\n",
      "Downloading yellow_tripdata_2021-05.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-05.parquet\n",
      "Downloading yellow_tripdata_2021-06.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-06.parquet\n",
      "Downloading yellow_tripdata_2021-07.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-07.parquet\n",
      "Downloading yellow_tripdata_2021-08.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-08.parquet\n",
      "Downloading yellow_tripdata_2021-09.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-09.parquet\n",
      "Downloading yellow_tripdata_2021-10.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-10.parquet\n",
      "Downloading yellow_tripdata_2021-11.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-11.parquet\n",
      "Downloading yellow_tripdata_2021-12.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2021-12.parquet\n",
      "Downloading yellow_tripdata_2022-01.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-01.parquet\n",
      "Downloading yellow_tripdata_2022-02.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-02.parquet\n",
      "Downloading yellow_tripdata_2022-03.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-03.parquet\n",
      "Downloading yellow_tripdata_2022-04.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-04.parquet\n",
      "Downloading yellow_tripdata_2022-05.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-05.parquet\n",
      "Downloading yellow_tripdata_2022-06.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-06.parquet\n",
      "Downloading yellow_tripdata_2022-07.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-07.parquet\n",
      "Downloading yellow_tripdata_2022-08.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-08.parquet\n",
      "Downloading yellow_tripdata_2022-09.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-09.parquet\n",
      "Downloading yellow_tripdata_2022-10.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-10.parquet\n",
      "Downloading yellow_tripdata_2022-11.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-11.parquet\n",
      "Downloading yellow_tripdata_2022-12.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2022-12.parquet\n",
      "Downloading yellow_tripdata_2023-01.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-01.parquet\n",
      "Downloading yellow_tripdata_2023-02.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-02.parquet\n",
      "Downloading yellow_tripdata_2023-03.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-03.parquet\n",
      "Downloading yellow_tripdata_2023-04.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-04.parquet\n",
      "Downloading yellow_tripdata_2023-05.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-05.parquet\n",
      "Downloading yellow_tripdata_2023-06.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-06.parquet\n",
      "Downloading yellow_tripdata_2023-07.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-07.parquet\n",
      "Downloading yellow_tripdata_2023-08.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-08.parquet\n",
      "Downloading yellow_tripdata_2023-09.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-09.parquet\n",
      "Downloading yellow_tripdata_2023-10.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-10.parquet\n",
      "Downloading yellow_tripdata_2023-11.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-11.parquet\n",
      "Downloading yellow_tripdata_2023-12.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2023-12.parquet\n",
      "Downloading yellow_tripdata_2024-01.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-01.parquet\n",
      "Downloading yellow_tripdata_2024-02.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-02.parquet\n",
      "Downloading yellow_tripdata_2024-03.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-03.parquet\n",
      "Downloading yellow_tripdata_2024-04.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-04.parquet\n",
      "Downloading yellow_tripdata_2024-05.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-05.parquet\n",
      "Downloading yellow_tripdata_2024-06.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-06.parquet\n",
      "Downloading yellow_tripdata_2024-07.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-07.parquet\n",
      "Downloading yellow_tripdata_2024-08.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-08.parquet\n",
      "Downloading yellow_tripdata_2024-09.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-09.parquet\n",
      "Downloading yellow_tripdata_2024-10.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-10.parquet\n",
      "Downloading yellow_tripdata_2024-11.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-11.parquet\n",
      "Downloading yellow_tripdata_2024-12.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2024-12.parquet\n",
      "Downloading yellow_tripdata_2025-01.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-01.parquet\n",
      "Downloading yellow_tripdata_2025-02.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-02.parquet\n",
      "Downloading yellow_tripdata_2025-03.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-03.parquet\n",
      "Downloading yellow_tripdata_2025-04.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-04.parquet\n",
      "Downloading yellow_tripdata_2025-05.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-05.parquet\n",
      "Downloading yellow_tripdata_2025-06.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-06.parquet\n",
      "Downloading yellow_tripdata_2025-07.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-07.parquet\n",
      "Downloading yellow_tripdata_2025-08.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-08.parquet\n",
      "Downloading yellow_tripdata_2025-09.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-09.parquet\n",
      "Downloading yellow_tripdata_2025-10.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-10.parquet\n",
      "Downloading yellow_tripdata_2025-11.parquet...\n",
      "  ✓ Saved to data\\yellow_tripdata_2025-11.parquet\n",
      "Downloading yellow_tripdata_2025-12.parquet...\n",
      "  ✗ Failed to download yellow_tripdata_2025-12.parquet: HTTP Error 403: Forbidden\n",
      "------------------------------------------------------------\n",
      "Download complete! 60 files available for analysis\n",
      "Successfully downloaded/found: 60 files\n"
     ]
    }
   ],
   "source": [
    "# Execute download\n",
    "print(f\"Starting download: {data_type} taxi data for the last {years_back} years\")\n",
    "print(f\"Method: {'Playwright (dynamic parsing)' if use_playwright else 'Direct (URL pattern)'}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "if use_playwright:\n",
    "    data_files = download_tlc_data_playwright(data_type, years_back, data_dir)\n",
    "else:\n",
    "    data_files = download_tlc_data_direct(data_type, years_back, data_dir)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Download complete! {len(data_files)} files available for analysis\")\n",
    "\n",
    "# Filter out files that don't exist (failed downloads)\n",
    "data_files = [f for f in data_files if Path(f).exists()]\n",
    "print(f\"Successfully downloaded/found: {len(data_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis with DuckDB\n",
    "\n",
    "Now that we have the data, let's analyze it using DuckDB. DuckDB can query Parquet files directly without loading them into memory, making it perfect for large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227a9a4386f9408288900873e35569dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mModuleNotFoundError\u001B[39m                       Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 17\u001B[39m\n\u001B[32m      6\u001B[39m \u001B[38;5;66;03m# Get basic statistics\u001B[39;00m\n\u001B[32m      7\u001B[39m query = \u001B[33mf\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[33mSELECT \u001B[39m\n\u001B[32m      9\u001B[39m \u001B[33m    COUNT(*) as total_trips,\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m     14\u001B[39m \u001B[33mFROM \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpattern\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\n\u001B[32m     15\u001B[39m \u001B[33m\u001B[39m\u001B[33m\"\"\"\u001B[39m\n\u001B[32m---> \u001B[39m\u001B[32m17\u001B[39m stats = \u001B[43mduckdb\u001B[49m\u001B[43m.\u001B[49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mpl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mDataset Statistics:\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     19\u001B[39m \u001B[38;5;28mprint\u001B[39m(stats)\n",
      "\u001B[31mModuleNotFoundError\u001B[39m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "# Query data directly from Parquet files using DuckDB\n",
    "# Use glob pattern to query all downloaded files at once\n",
    "if data_files:\n",
    "    pattern = f\"{data_dir}/{data_type}_tripdata_*.parquet\"\n",
    "    \n",
    "    # Get basic statistics\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_trips,\n",
    "        MIN(tpep_pickup_datetime) as earliest_trip,\n",
    "        MAX(tpep_pickup_datetime) as latest_trip,\n",
    "        AVG(trip_distance) as avg_distance,\n",
    "        AVG(total_amount) as avg_fare\n",
    "    FROM '{pattern}'\n",
    "    \"\"\"\n",
    "    \n",
    "    stats = duckdb.sql(query).pl()\n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(stats)\n",
    "else:\n",
    "    print(\"No data files available for analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze trips by month and year - convert to Polars for further processing\nif data_files:\n    query = f\"\"\"\n    SELECT \n        YEAR(tpep_pickup_datetime) as year,\n        MONTH(tpep_pickup_datetime) as month,\n        COUNT(*) as trip_count,\n        AVG(trip_distance) as avg_distance,\n        AVG(total_amount) as avg_fare,\n        SUM(total_amount) as total_revenue\n    FROM '{pattern}'\n    GROUP BY year, month\n    ORDER BY year, month\n    \"\"\"\n    \n    monthly_stats = duckdb.sql(query).pl()\n    \n    # Use Polars to add year-month string for better display\n    monthly_stats = monthly_stats.with_columns(\n        (pl.col(\"year\").cast(str) + \"-\" + pl.col(\"month\").cast(str).str.zfill(2)).alias(\"year_month\")\n    )\n    \n    print(f\"\\nMonthly Statistics ({len(monthly_stats)} months):\")\n    print(monthly_stats.head(12))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze pickup locations (Location IDs for congestion pricing zones)\n",
    "# Manhattan's congestion pricing zone is roughly LocationIDs in certain ranges\n",
    "if data_files:\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        PULocationID as pickup_location,\n",
    "        COUNT(*) as trip_count,\n",
    "        AVG(trip_distance) as avg_distance,\n",
    "        AVG(total_amount) as avg_fare\n",
    "    FROM '{pattern}'\n",
    "    WHERE PULocationID IS NOT NULL\n",
    "    GROUP BY PULocationID\n",
    "    ORDER BY trip_count DESC\n",
    "    LIMIT 20\n",
    "    \"\"\"\n",
    "    \n",
    "    top_locations = duckdb.sql(query).pl()\n",
    "    \n",
    "    print(\"\\nTop 20 Pickup Locations by Trip Count:\")\n",
    "    print(top_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save aggregated results to outputs directory\n",
    "Path(\"outputs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if data_files and len(monthly_stats) > 0:\n",
    "    # Save monthly statistics\n",
    "    monthly_stats.write_parquet(output_path)\n",
    "    print(f\"\\nResults saved to {output_path}\")\n",
    "    print(f\"Total months analyzed: {len(monthly_stats)}\")\n",
    "    print(f\"Date range: {monthly_stats['year_month'].min()} to {monthly_stats['year_month'].max()}\")\n",
    "else:\n",
    "    print(\"\\nNo results to save\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nyc-congestion-pricing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
